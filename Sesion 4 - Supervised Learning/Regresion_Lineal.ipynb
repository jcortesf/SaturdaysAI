{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 style = \"text-align: center;\"> 1. Análisis de regresión </h1>\n",
    "El análisis de regresión es el campo de las matemáticas donde el objetivo es encontrar una función que se correlacione mejor con un conjunto de datos. Digamos que tenemos un conjunto de datos que contiene $ n $ puntos de datos;\n",
    "\n",
    "<p style=\"text-align: center;\">$ X = ( x^{(1)}, x^{(2)}, .., x^{(n)} ) $. </p>\n",
    "\n",
    "Para cada uno de estos puntos de datos (de entrada) hay un valor $ y ^ {(i)} $ correspondiente (de salida).\n",
    "\n",
    "Aquí, los puntos de datos $ x $ se denominan variables independientes y $ y $ la variable dependiente;\n",
    "el valor de $ y ^ {(i)} $ depende del valor de $ x ^ {(i)} $, mientras que el valor de $ x ^ {(i)} $ puede elegirse libremente sin ninguna restricción impuesta por cualquier otra variable.\n",
    "\n",
    "El objetivo del análisis de regresión es encontrar una función $ f (X) $ que pueda describir mejor la correlación entre $ X $ y $ Y $. En el campo del aprendizaje automático, esta función se denomina función de hipótesis y se denota como $ h _ {\\theta} (x) $. Si podemos encontrar dicha función, podemos decir que hemos construido con éxito un modelo de regresión.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"img/correlation_function.png\">\n",
    "\n",
    "Si los datos de entrada viven en un espacio 2D, esto se reduce a encontrar una curva que se ajuste a los puntos de datos. En el caso de 3D tenemos que encontrar un plano y en dimensiones superiores un hiperplano.\n",
    "\n",
    "Para dar un ejemplo, digamos que estamos tratando de encontrar un modelo predictivo para el éxito de los estudiantes del curso de Saturday's AI. Tenemos un conjunto de datos $ Y $ que contiene la calificación final de $ n $ estudiantes. El conjunto de datos $ X $ contiene los valores de las variables independientes. Nuestra suposición inicial es que la calificación final solo depende del tiempo de estudio. La variable $ x ^ {(i)} $ por lo tanto indica cuántas horas ha estudiado el estudiante $ i $. Lo primero que haríamos es visualizar estos datos:\n",
    "\n",
    "<img src=\"img/regression_left2-350x288.png\">\n",
    "\n",
    "Si los resultados se parecen a la figura de la izquierda, entonces no tenemos suerte. Parece que los puntos se distribuyen aleatoriamente y no existe ninguna correlación entre $ Y $ y $ X $. Sin embargo, si se parece a la figura de la derecha, probablemente haya una fuerte correlación y podamos empezar a buscar la función que describe esta correlación.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Esta función podría ser, por ejemplo:\n",
    "<p style=\"text-align: center;\">$ h_{\\theta}(X) =  \\theta_0+ \\theta_1 \\cdot x $</p>\n",
    "<p style=\"text-align: left;\">o</p>\n",
    "<p style=\"text-align: center;\">$ h_{\\theta}(X) = \\theta_0 + \\theta_1 \\cdot x^2 $</p>\n",
    "donde $ \\theta $ son los parámetros dependientes de nuestro modelo.\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "<h2> 1.2 Regresión multivariante </h2>\n",
    "<br>\n",
    "Al evaluar los resultados de la sección anterior, podemos encontrar resultados insatisfactorios; la función no se correlaciona suficientemente con los puntos de datos. Nuestra suposición inicial probablemente no esté completa. No basta con tener en cuenta solo el tiempo de estudio.\n",
    "\n",
    "La nota final no solo depende del tiempo de estudio, sino también de cuánto hayan dormido los alumnos la noche anterior al examen. Ahora el conjunto de datos contiene una variable adicional que representa el tiempo de sueño.\n",
    "\n",
    "Nuestro conjunto de datos viene dado por $ X = ((x_1 ^ {(1)}, x_2 ^ {(1)}), (x_1 ^ {(2)}, x_2 ^ {(2)}), ..., ( x_1 ^ {(n)}, x_2 ^ {(n)})) $. En este conjunto de datos, $ x_1 ^ {(i)} $ indica cuántas horas ha estudiado el estudiante $ i $ y $ x_2 ^ {(i)} $ indica cuántas horas ha dormido.\n",
    "\n",
    "<img src=\"img/regression_multi.png\">\n",
    "\n",
    "Este es un ejemplo de Regresión multivariante. La función debe incluir ambas variables. Por ejemplo:\n",
    "<p style=\"text-align: center;\">$ h_{\\theta}(x) = \\theta_0 + \\theta_1 \\cdot x_1 + \\theta_2 \\cdot x_2$</p>\n",
    "or\n",
    "<p style=\"text-align: center;\">$ h_{\\theta}(x) = \\theta_0 + \\theta_1 \\cdot x_1 + \\theta_2 \\cdot x_2^3 $.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2> 1.3 Regresión lineal vs. no lineal </h2>\n",
    "<br>\n",
    "Todos los ejemplos anteriores son ejemplos de regresión lineal. Hemos visto que en algunos casos $ y ^ {(i)} $ depende de una forma lineal de $ x ^ {(i)} $, pero también puede depender de alguna potencia de $ x ^ {(i)} $ , o en el registro o cualquier otra forma de $ x ^ {(i)} $. Sin embargo, en todos los casos su dependencia de los parámetros $ \\theta $ fue lineal.\n",
    "\n",
    "Entonces, lo que hace que la regresión lineal sea lineal no es que $ Y $ dependa de forma lineal de $ X $, sino que depende de forma lineal de $ \\theta $.\n",
    "$ Y $ debe ser lineal con respecto a los parámetros del modelo $ \\theta $. Matemáticamente hablando, debe satisfacer el [principio de superposición](http://www.cut-the-knot.org/do_you_know/superposition.shtml).\n",
    "\n",
    "Ejemplos de regresión no lineal serían:\n",
    "<p style=\"text-align: center;\">$ h_{\\theta}(x) = \\theta_0 + x_1^{\\theta_1} $</p>\n",
    "or\n",
    "<p style=\"text-align: center;\">$ h_{\\theta}(x) = \\theta_0 + \\theta_1 / x_1 $</p>\n",
    "<br>\n",
    "La razón por la que se hace la distinción entre regresión lineal y no lineal es que los problemas de regresión no lineal son más difíciles de resolver y, por lo tanto, se necesitan más algoritmos computacionales intensivos.\n",
    "\n",
    "Los modelos de regresión lineal se pueden escribir como un sistema lineal de ecuaciones, que se puede resolver encontrando la solución de forma cerrada $ \\theta = (X ^ TX) ^ {- 1} X ^ TY $ con Álgebra lineal. \n",
    "\n",
    "Consulte <a href=\"http://www.stat.purdue.edu/~jennings/stat514/stat512notes/topic3.pdf\" target=\"_blank\"> estas </a> notas estadísticas para obtener más información sobre la resolución de modelos lineales con álgebra.\n",
    "\n",
    "Como se discutió anteriormente, esta solución de forma cerrada solo se puede encontrar para problemas de regresión lineal. Sin embargo, incluso cuando el problema es de naturaleza lineal, debemos tener en cuenta que el cálculo de la inversa de una matriz de $ n $ por $ n $ tiene una complejidad de tiempo de $ O (n ^ 3) $. Esto significa que para conjuntos de datos grandes ($ n \\gt 10.000 $) encontrar la solución de forma cerrada llevará más tiempo que resolverla iterativamente (método de descenso de gradiente) como se hace para problemas no lineales. Por lo tanto, generalmente se prefiere resolverlo de manera iterativa para conjuntos de datos más grandes, incluso si se trata de un problema lineal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 1.4 Gradient Descend </h2>\n",
    "<br>\n",
    "El método Gradient Descent es una técnica de optimización general en la que intentamos encontrar el valor de los parámetros $ \\theta $ con un enfoque iterativo.\n",
    "\n",
    "Primero, construimos una <a href=\"https://en.wikipedia.org/wiki/Loss_function\" target=\"_blank\"> función de costo </a> (también conocida como función de pérdida o función de error) que da la diferencia entre los valores de la función de hipótesis $ h _ {\\theta} (x) $ (los valores que espera que tenga $ Y $ con los valores actuales de $ \\theta $) y los valores reales de $ Y $. Cuanto mejor sea su estimación de $ \\theta $, mejor se acercarán los valores de $ h _ {\\theta} (x) $ a los valores de $ Y $ y menor será la función de costo.\n",
    "\n",
    "Por lo general, la función de costo se expresa como el error al cuadrado de la diferencia entre estas funciones:\n",
    "<p style=\"text-align: center;\">$ J(x) = \\frac{1}{2n} \\sum_i^n ( h_{\\theta}(x^{(i)}) - y^{(i)} )^2 $</p>\n",
    "<br>\n",
    "\n",
    "En cada iteración elegimos nuevos valores para los parámetros $ \\theta $, y nos movemos hacia los valores 'verdaderos' de estos parámetros, es decir, los valores que hacen que esta función de costo sea lo más pequeña posible. La dirección en la que tenemos que movernos es la dirección del gradiente negativo;\n",
    "\n",
    "<p style=\"text-align: center;\"> $ \\Delta\\theta = - \\alpha \\frac{d}{d\\theta} J(x) $.</p>\n",
    "\n",
    "La razón de esto es que el valor de una función disminuye más rápidamente si nos movemos hacia la dirección del gradiente negativo (la <a href=\"https://es.wikipedia.org/wiki/Derivada_direccional\" target=\"_blank\">  derivada direccional</a> es máximo en la dirección del gradiente).\n",
    "\n",
    "Teniendo todo esto en cuenta, así es como funciona el descenso de gradientes:\n",
    "<ul>\n",
    "<li> Haga una suposición inicial pero inteligente de los valores de los parámetros $ \\theta $. </li>\n",
    "<li> Siga iterando mientras el valor de la función de costo no cumpla con sus criterios:\n",
    "<ul>\n",
    "<li> Con los valores actuales de $ \\theta $, calcule el gradiente de la función de costo J ($ \\Delta \\theta = - \\alpha \\frac {d} {d \\theta} J (x) $). </ li>\n",
    "<li> Actualice los valores de los parámetros $ \\theta: = \\theta + \\alpha \\Delta \\theta $ </li>\n",
    "<li> Complete estos nuevos valores en la función de hipótesis y calcule nuevamente el valor de la función de costo; </li>\n",
    "</ul>\n",
    "</li>\n",
    "</ul>\n",
    "\n",
    "Tan importante como la estimación inicial de los parámetros es el valor que elija para la tasa de aprendizaje $ \\alpha $. Esta tasa de aprendizaje determina qué tan rápido se mueve a lo largo de la pendiente del gradiente. Si el valor seleccionado de esta tasa de aprendizaje es demasiado pequeño, se necesitarán demasiadas iteraciones antes de alcanzar sus criterios de convergencia. Si este valor es demasiado grande, es posible que se sobrepase y no converja."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style = \"text-align: center;\"> 2. Implementación </h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"students.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"Hours\",y=\"Scores\",data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,theta):\n",
    "    return np.dot(X,theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost(X,theta,y):\n",
    "    prediction = predict(X,theta)\n",
    "    return ((prediction - y)**2).mean()/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abline(X,theta,y):\n",
    "    \"\"\"Plot a line from slope and intercept\"\"\"\n",
    "    y_vals = predict(X,theta)\n",
    "    plt.xlim(4, 10)\n",
    "    plt.ylim(10, 100)\n",
    "    plt.xlabel('Hours')\n",
    "    plt.ylabel('Score')\n",
    "    plt.gca().set_aspect(0.1, adjustable='datalim')\n",
    "    plt.plot(X,y,'.')\n",
    "    plt.plot(X, y_vals, '-')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_linear_regression(alpha=0.047,iter=5000):\n",
    "    theta0 = []\n",
    "    theta1 = []\n",
    "    costs = []\n",
    "    predictor = df[\"Hours\"]\n",
    "    X = np.column_stack((np.zeros(len(predictor)),predictor))\n",
    "    y = df[\"Scores\"]\n",
    "    theta = np.zeros(2)\n",
    "    for i in range(iter):\n",
    "        pred = predict(X,theta)\n",
    "        t0 = theta[0] - alpha *(pred - y).mean()\n",
    "        t1 = theta[1] - alpha *((pred - y)* X[:,1]).mean()\n",
    "        \n",
    "        theta = np.array([t0,t1])\n",
    "        J = calculate_cost(X,theta,y)\n",
    "        theta0.append(t0)\n",
    "        theta1.append(t1)\n",
    "        costs.append(J)\n",
    "        if i%1000==0:\n",
    "            print(f\"Iteration: {i+1},Cost = {J},theta = {theta}\")\n",
    "            abline(X,theta,y)\n",
    "    print(f'theta0 = {np.mean(theta0)}\\ntheta1 = {np.mean(theta1)}\\nCosts = {np.mean(costs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_descent_linear_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "X = df[[\"Hours\"]]\n",
    "y = df[\"Scores\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linear regression object\n",
    "regr = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "regr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regr.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)\n",
    "# The mean squared error\n",
    "print('Mean squared error: %.2f'\n",
    "      % mean_squared_error(y, y_pred))\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.2f'\n",
    "      % r2_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot outputs\n",
    "plt.scatter(X, y,  color='orange')\n",
    "plt.plot(X, y_pred, color='blue', linewidth=3)\n",
    "plt.xlabel('Hours')\n",
    "plt.ylabel('Score')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
